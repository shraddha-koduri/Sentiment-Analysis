{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\"></a>\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> LLM Llama - Engineering prompt Sentiment analysis climate</b></div>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://img.freepik.com/fotos-gratis/chamines-contra-uma-paisagem-industrial-de-ceu-limpo_91128-4692.jpg?t=st=1727545980~exp=1727549580~hmac=3ad404a9538b0cff5eed7ac466180b5e6b5fffa776daa61b06c27e8b8e3e6f6e&w=740\" />\n",
        "</div>\n",
        "\n",
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 1 - Business problem</b></div>\n",
        "\n",
        "### Business Problem: Sentiment Analysis of Tweets on Climate Change\n",
        "\n",
        "**Objective:**\n",
        "The goal is to classify tweets about \"Climate Change\" into three sentiment categories: positive, neutral, and negative, using a pre-trained large language model (LLM), specifically LLaMA. This classification will help identify public opinion trends and reactions to climate change over time. The insights gained can be used by environmental organizations, policymakers, and businesses to better understand public sentiment and develop strategies for communication, engagement, and policy making.\n",
        "\n",
        "**Key Questions:**\n",
        "1. What are the predominant sentiments (positive, neutral, or negative) expressed in daily discussions on Twitter about climate change?\n",
        "2. How does public sentiment about climate change evolve over time?\n",
        "3. Are there specific events or time periods that trigger noticeable shifts in sentiment?\n",
        "4. Which factors (e.g., hashtags, keywords, influencers) are associated with positive or negative sentiment in the climate change conversation?\n",
        "\n",
        "**Dataset Overview:**\n",
        "- The dataset includes daily tweets containing the keyword \"Climate Change\" from January 1, 2022, to July 19, 2022.\n",
        "- It consists of 11 columns, likely containing text data, user information, tweet metadata (e.g., likes, retweets), and timestamps.\n",
        "\n",
        "**Steps to Solve the Problem:**\n",
        "\n",
        "1. **Data Preparation and Exploration**:\n",
        "   - Perform data cleaning (removing duplicates, handling missing values, cleaning up the tweet text by removing links, mentions, hashtags, etc.).\n",
        "   - Conduct exploratory data analysis to understand the distribution of tweets over time and any potential trends in volume.\n",
        "\n",
        "2. **Preprocessing for LLaMA**:\n",
        "   - Tokenize and preprocess the tweet text to be compatible with the LLaMA model.\n",
        "   - Optionally fine-tune the LLaMA model using labeled data if available, or use it directly for sentiment classification through prompt-based techniques.\n",
        "\n",
        "3. **Sentiment Classification**:\n",
        "   - Use LLaMA to classify each tweet into one of three categories: positive, neutral, or negative.\n",
        "   - Generate prompts that guide the LLaMA model to assess the sentiment of each tweet.\n",
        "\n",
        "4. **Evaluation and Metrics**:\n",
        "   - Evaluate the model's performance using metrics like accuracy, precision, recall, F1-score, and confusion matrices.\n",
        "   - Analyze the performance of the sentiment classification to ensure reliable results.\n",
        "\n",
        "5. **Visualization and Insights**:\n",
        "   - Create visualizations that show the sentiment trends over time.\n",
        "   - Identify spikes in sentiment around significant events or milestones in the climate change discussion.\n",
        "\n",
        "6. **Business Impact**:\n",
        "   - Organizations can use the sentiment data to adjust communication strategies based on public opinion trends.\n",
        "   - Governments and policymakers can use sentiment insights to craft targeted interventions or campaigns.\n",
        "   - Businesses in the green energy sector can leverage these insights for marketing strategies.\n",
        "\n",
        "**Expected Outcome**:\n",
        "By successfully classifying tweets into positive, neutral, or negative sentiments, the analysis will provide a clear picture of public opinion regarding climate change. This will offer actionable insights to various stakeholders involved in climate action.\n",
        "\n",
        "This business problem aims to harness the power of LLaMA for sentiment analysis in a highly relevant and timely context—public discourse on climate change."
      ],
      "metadata": {
        "id": "kd79-IN0PfI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGwaMlLsPNxX"
      },
      "outputs": [],
      "source": [
        "# Installing the latest versions of the Hugging Face Transformers library and Accelerate library\n",
        "# Transformers: a library for natural language processing tasks like text classification, translation, etc.\n",
        "# Accelerate: used to easily scale models across different hardware setups (CPU, GPU, multi-GPU, etc.)\n",
        "# Install the bitsandbytes library\n",
        "# bitsandbytes: a lightweight library that allows running large language models with fewer bits, enabling memory-efficient model training and inference.\n",
        "\n",
        "# Installing packages\n",
        "!pip install watermark\n",
        "!pip install -U transformers accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install torch\n",
        "!pip install spacy\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary resources from NLTK\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')  # Tokenizer models\n",
        "nltk.download('stopwords')  # Stopwords data\n",
        "nltk.download('wordnet')  # WordNet lemmatizer data\n",
        "\n",
        "# Download Spacy language models for English\n",
        "# English model\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "2NxcRIGTPqlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import of libraries\n",
        "\n",
        "# System libraries\n",
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# Library for file manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas\n",
        "\n",
        "# Data visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as pl\n",
        "import matplotlib as m\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as STOP_WORDS_EN  # English stopwords\n",
        "\n",
        "# Load the English language model from spaCy\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Configuration for graph width and layout\n",
        "sns.set_theme(style='whitegrid')\n",
        "palette='viridis'\n",
        "\n",
        "# Importing necessary libraries from PyTorch and Hugging Face Transformers\n",
        "# PyTorch is a deep learning framework used for model training and inference\n",
        "import torch\n",
        "\n",
        "# AutoTokenizer: Automatically loads a pre-trained tokenizer for encoding text\n",
        "# AutoModelForCausalLM: Loads a pre-trained model for causal language modeling (e.g., for text generation)\n",
        "# pipeline: Provides an easy-to-use interface to perform tasks like text generation, sentiment analysis, etc.\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Warnings remove alerts\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Python version\n",
        "from platform import python_version\n",
        "print('Python version in this Jupyter Notebook:', python_version())\n",
        "\n",
        "# Load library versions\n",
        "import watermark\n",
        "\n",
        "# Library versions\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Library versions\" --iversions"
      ],
      "metadata": {
        "id": "Q0IzJb_FPsdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPU In LLM**"
      ],
      "metadata": {
        "id": "ddagxA9VUsim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "bzYXOutoU3FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -a"
      ],
      "metadata": {
        "id": "NBVHfd3gXaES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "id": "dZwDDNsPXdcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "BX6Vnz70VqJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the summary of CUDA memory usage for the specified device\n",
        "print(torch.cuda.memory_summary(device=torch.device('cuda')))"
      ],
      "metadata": {
        "id": "50gfwXwaVBs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 2 - Database</b></div>"
      ],
      "metadata": {
        "id": "JQ0hqWkxPujJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Aqui vamos só carregar 1.000 linhas o dataset tem 9.050 linhas ou seja modelo vai demorar para processar pelo menos 3 horas estamos usando Google Colab existe um limite de horas na GPU gratuito.\n",
        "\n",
        "- Portando vamos limitar o dataset para 1.000 linhas."
      ],
      "metadata": {
        "id": "JF0Xs83taVMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**- Link:** [Base dados - Kaggle](https://www.kaggle.com/datasets/die9origephit/climate-change-tweets)"
      ],
      "metadata": {
        "id": "sGeqQwB3rFbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando dataset\n",
        "df = pd.read_csv(\"/content/Climate change_2022-1-17_2022-7-19.csv\", nrows=100)\n",
        "df = df[['UserScreenName', 'UserName', 'Text', 'Embedded_text']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "FJJT4Lm2aNaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5ZjaXLeAPsip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "-oFsX5AYQOV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "onWXE0HbQOYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Tu0eNfVLQObe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "ZCzGO-BwPsl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 3 - Preprocessing Text</b></div>"
      ],
      "metadata": {
        "id": "VKol4khkQSCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the \"Text\" column is a string\n",
        "df['Text'] = df['Text'].astype(str)\n",
        "\n",
        "# Data info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ucnM1IccQTu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def limpar_texto(texto):\n",
        "    # Remove URLs\n",
        "    texto = re.sub(r'http\\S+|www.\\S+', '', texto)\n",
        "\n",
        "    # Remove mentions (@user)\n",
        "    texto = re.sub(r'@\\w+', '', texto)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    texto = re.sub(r'#\\w+', '', texto)\n",
        "\n",
        "    # Remove emojis and non-ASCII characters\n",
        "    texto = texto.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    # Remove punctuation\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Convert to lowercase\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    palavras = word_tokenize(texto)\n",
        "\n",
        "    # Remove Portuguese stopwords\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    palavras = [palavra for palavra in palavras if palavra not in stop_words]\n",
        "\n",
        "    # Join the words back together\n",
        "    texto_limpo = ' '.join(palavras)\n",
        "\n",
        "    return texto_limpo\n",
        "\n",
        "# Apply the cleaning function to the 'Embedded_text' column\n",
        "df['Text_Limpo'] = df['Embedded_text'].apply(limpar_texto)\n",
        "\n",
        "# List of columns to be removed\n",
        "colunas_para_remover = ['UserName',\n",
        "                        'Timestamp',\n",
        "                        'Emojis',\n",
        "                        'Comments',\n",
        "                        'Likes',\n",
        "                        'Retweets',\n",
        "                        'Image link',\n",
        "                        'Tweet URL']\n",
        "\n",
        "# Check which columns exist in the dataframe and are in the removal list\n",
        "colunas_existentes = [col for col in colunas_para_remover if col in df.columns]\n",
        "\n",
        "# Drop the existing columns from the dataframe\n",
        "df.drop(columns=colunas_existentes, inplace=True)\n",
        "\n",
        "# Display the dataset\n",
        "df = df[[\"Text\", \"Text_Limpo\"]]\n",
        "df.Text_Limpo.head(n=20)"
      ],
      "metadata": {
        "id": "JXWfQZREQZkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for removing stopwords\n",
        "def remover_stopwords_nltk(tokens):\n",
        "    \"\"\"\n",
        "    Remove stopwords from a list of tokens using NLTK.\n",
        "\n",
        "    Parameters:\n",
        "    tokens (list): List of tokens (words) to be filtered.\n",
        "\n",
        "    Returns:\n",
        "    list: List of tokens without stopwords.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_filtrados = [token for token in tokens if token.lower() not in stop_words]\n",
        "    return tokens_filtrados\n",
        "\n",
        "# Function for tokenization\n",
        "def tokenizar_texto_spacy(texto, modelo):\n",
        "    \"\"\"\n",
        "    Tokenize text using spaCy.\n",
        "\n",
        "    Parameters:\n",
        "    texto (str): Text to be tokenized.\n",
        "    modelo (spaCy model): spaCy language model.\n",
        "\n",
        "    Returns:\n",
        "    list: List of tokens.\n",
        "    \"\"\"\n",
        "    if pd.isnull(texto):\n",
        "        return []\n",
        "\n",
        "    # Process the text with the spaCy model\n",
        "    doc = modelo(texto)\n",
        "\n",
        "    # Extract alphabetic tokens only\n",
        "    tokens = [token.text.lower() for token in doc if token.is_alpha]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function for Pre-Processing with spacy\n",
        "def processar_texto_en_spacy(texto):\n",
        "    \"\"\"\n",
        "    Process text by cleaning, tokenizing, removing stopwords, and lemmatizing using spaCy.\n",
        "\n",
        "    Parameters:\n",
        "    texto (str): Text to be processed.\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing the tokens and lemmas of the text.\n",
        "    \"\"\"\n",
        "    if pd.isnull(texto):\n",
        "        return {\"tokens_spacy\": [], \"lemmas_spacy\": []}\n",
        "\n",
        "    # Remove URLs\n",
        "    texto = re.sub(r'http\\S+|www\\.\\S+', '', texto)\n",
        "\n",
        "    # Remove mentions (@user)\n",
        "    texto = re.sub(r'@\\w+', '', texto)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    texto = re.sub(r'#\\w+', '', texto)\n",
        "\n",
        "    # Remove emojis and non-ASCII characters\n",
        "    texto = texto.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    # Remove punctuation\n",
        "    texto = texto.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Convert to lowercase\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Tokenize with spaCy\n",
        "    tokens_spacy = tokenizar_texto_spacy(texto, nlp_en)\n",
        "\n",
        "    # Remove stopwords using NLTK\n",
        "    tokens_filtrados_spacy = remover_stopwords_nltk(tokens_spacy)\n",
        "\n",
        "    # Lemmatization using spaCy\n",
        "    doc = nlp_en(' '.join(tokens_filtrados_spacy))\n",
        "    lemas_spacy = [token.lemma_ for token in doc]\n",
        "\n",
        "    return {\"tokens_spacy\": tokens_filtrados_spacy, \"lemmas_spacy\": lemas_spacy}\n",
        "\n",
        "# Apply the text processing function using spaCy\n",
        "df[['Tokens_SpaCy', 'Lemas_SpaCy']] = df['Text_Limpo'].apply(lambda x: pd.Series(processar_texto_en_spacy(x)))\n",
        "\n",
        "# Display the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WZfhMdRrQZ1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting columns from the dataset\n",
        "columns_to_remove = ['UserName', 'Timestamp', 'Emojis', 'Comments', 'Likes', 'Retweets', 'Image link', 'Tweet URL']\n",
        "existing_columns = [col for col in columns_to_remove if col in df.columns]\n",
        "df.drop(columns=existing_columns, inplace=True)\n",
        "df.Text_Limpo.head(n=20)"
      ],
      "metadata": {
        "id": "9Bv5Ve8GQZ4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 4 - Exploratory data analysis</b></div>"
      ],
      "metadata": {
        "id": "Y8y14DQLQ1k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining all texts from the 'Text_Limpo' column into a single string\n",
        "text = \" \".join(review for review in df.Text_Limpo)\n",
        "\n",
        "# Defining additional stopwords\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# Displaying the word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.figure(figsize=(20.5, 10))\n",
        "plt.title(\"Word cloud - General clean text\")\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CP9aeie_Q2e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all tokens from the 'Tokens_SpaCy' column into a single list\n",
        "all_tokens = [token for tokens in df['Tokens_SpaCy'] for token in tokens]\n",
        "\n",
        "# Count the frequency of each token using Counter\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# Get the top 20 most common tokens\n",
        "common_tokens = token_counts.most_common(20)  # Limiting the result to the top 20 tokens\n",
        "\n",
        "# Separate the tokens and their frequencies for plotting\n",
        "tokens, frequencies = zip(*common_tokens)\n",
        "\n",
        "# Create a bar plot to visualize the most frequent tokens\n",
        "plt.figure(figsize=(12, 6))  # Set figure size\n",
        "sns.barplot(x=list(frequencies), y=list(tokens), palette='Set2')  # Plot using seaborn with 'husl' color palette\n",
        "\n",
        "# Set title and labels with improved readability\n",
        "plt.title('Top 20 Most Common Tokens', fontsize=16)  # Title for the plot\n",
        "plt.xlabel('Frequency', fontsize=14)  # X-axis label\n",
        "plt.ylabel('Tokens', fontsize=14)  # Y-axis label\n",
        "\n",
        "# Add gridlines to the x-axis for better readability\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)  # Dashed gridlines on the x-axis\n",
        "\n",
        "# Adjust the layout to ensure the plot elements fit well\n",
        "plt.tight_layout()\n",
        "\n",
        "# Remove default seaborn grid\n",
        "plt.grid(False)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n_25XYDGQ4M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all tokens into a single list\n",
        "all_tokens = [token for tokens in df['Lemas_SpaCy'] for token in tokens]\n",
        "\n",
        "# Count the frequency of tokens\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# Get the top 20 most common tokens\n",
        "common_tokens = token_counts.most_common(35)  # Limiting to top 35\n",
        "\n",
        "# Separate tokens and their frequencies\n",
        "tokens, frequencies = zip(*common_tokens)\n",
        "\n",
        "# Create a bar plot for the most frequent tokens\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=list(frequencies), y=list(tokens), palette='Set2')  # Changed palette\n",
        "\n",
        "# Improved title and axis labels\n",
        "plt.title('Top 35 Most Common Lemmatization ', fontsize=16)\n",
        "plt.xlabel('Frequency', fontsize=14)\n",
        "plt.ylabel('Tokens', fontsize=14)\n",
        "\n",
        "# Add gridlines for easier reading of bar heights\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()  # Ensure layout is clean and labels fit well\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cexBF1oGQ4Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 5 - Model LLM</b></div>"
      ],
      "metadata": {
        "id": "gwbvOKGdQ63_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "bbryxjTOStZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the local model directory\n",
        "# In this case, the model is stored locally at the specified path, which likely\n",
        "# contains the necessary files for a LLaMA-based model\n",
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "0KCESJHZTjp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a GPU is available\n",
        "# If a GPU is available, it will use \"cuda\"; otherwise, it will default to \"cpu\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "70VNJDzoQ7Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the local model directory\n",
        "# In this case, the model is stored locally at the specified path, which likely\n",
        "# contains the necessary files for a LLaMA-based model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the model with automatic device mapping to save memory\n",
        "# The model is loaded from the pre-trained local path and uses float16 precision (half-precision) to save memory.\n",
        "# The \"device_map='auto'\" allows for automatic distribution of the model across available hardware (e.g., GPU, CPU) to optimize memory usage.\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map=\"auto\")"
      ],
      "metadata": {
        "id": "4ege7mvSR2xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "# This loads the pre-trained tokenizer from the specified local model path.\n",
        "# The tokenizer is responsible for converting text into token IDs that the model can process.\n",
        "from transformers import pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
      ],
      "metadata": {
        "id": "whunN-veR117"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Pipeline Initialization\n",
        "print(\"Initializing the zero-shot classification pipeline...\")\n",
        "classifier = pipeline('zero-shot-classification',\n",
        "                      model=model,\n",
        "                      tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "YoTVy407X2Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 8.1 - Engineering prompt examples</b></div>"
      ],
      "metadata": {
        "id": "lSq9xBCpR6qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Defining Sentiment Labels\n",
        "labels = [\"Positive\", \"Negative\", \"Neutral\"]"
      ],
      "metadata": {
        "id": "hackBqxtX4pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "FGG583UOX4yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text based on a prompt\n",
        "# This function takes a text prompt and generates a continuation of the text.\n",
        "# The max_length parameter controls the maximum number of tokens generated.\n",
        "def generate_text(prompt, max_length=150):\n",
        "    # Tokenize the input prompt and move it to the model's device (e.g., GPU or CPU)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate text from the model based on the input token IDs\n",
        "    outputs = model.generate(inputs.input_ids, max_length=max_length)\n",
        "\n",
        "    # Decode the output tokens back into a human-readable string, skipping special tokens\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "2Pp355BrR5he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt the LLM 1\n",
        "\n",
        "# Input prompt for generating text\n",
        "prompt = \"What is exoplanet discovery ?\"\n",
        "\n",
        "# Generate text based on the prompt\n",
        "response = generate_text(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(response)"
      ],
      "metadata": {
        "id": "peHJMgfpR8K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt the LLM 2\n",
        "\n",
        "# Input prompt for generating text\n",
        "prompt = \"What was the James Webb Telescope looking for?\"\n",
        "\n",
        "# Generate text based on the prompt\n",
        "response = generate_text(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(response)"
      ],
      "metadata": {
        "id": "7AhTMZWcR8_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt the LLM 4\n",
        "\n",
        "# Input prompt for generating text\n",
        "prompt = (\n",
        "    \"You are a helpful and professional customer service assistant for a retail company. \"\n",
        "    \"Respond to the following customer inquiry in a friendly and efficient manner:\\n\\n\"\n",
        "    \"Customer Inquiry: \\\"I received my order today, but one of the items is damaged. What should I do?\\\"\\n\\n\"\n",
        "    \"Response:\"\n",
        ")\n",
        "\n",
        "# Generate text based on the prompt\n",
        "response = generate_text(prompt)\n",
        "\n",
        "# Print the generated response\n",
        "print(response)"
      ],
      "metadata": {
        "id": "gKfGzhYKR934"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 9 - Engineering prompt no dataset</b></div>"
      ],
      "metadata": {
        "id": "PScoDFm_R-uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Check if the \"Text\" column exists\n",
        "if 'Text_Limpo' not in df.columns:\n",
        "    raise ValueError(\"The dataset must contain a column named 'Text'.\")\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "\n",
        "print(\"Starting sentiment classification...\")\n",
        "for index, row in df.iterrows():\n",
        "    text = row['Text_Limpo']\n",
        "\n",
        "    # Create a specific prompt for each text\n",
        "    prompt = (\n",
        "        f\"Analyze the sentiment of the following text and classify it as Positive, Negative, or Neutral.\\n\"\n",
        "        f\"Text: \\\"{text}\\\"\\n\"\n",
        "        f\"Sentiment:\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Perform zero-shot classification\n",
        "        result = classifier(\n",
        "            sequences=text,\n",
        "            candidate_labels=labels,\n",
        "            hypothesis_template=\"This text expresses a {} sentiment.\"\n",
        "        )\n",
        "\n",
        "        # Get the label with the highest score\n",
        "        sentiment = result['labels'][0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {text[:30]}... - {e}\")\n",
        "        sentiment = \"Error\"\n",
        "\n",
        "    results.append(sentiment)\n",
        "\n",
        "    # Optional: Display progress every 100 iterations\n",
        "    if (index + 1) % 100 == 0:\n",
        "        print(f\"{index + 1} texts processed...\")"
      ],
      "metadata": {
        "id": "uOgsSP7AR_9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column for sentiment analysis results to the DataFrame\n",
        "df['Sentiment_LLM'] = results\n",
        "\n",
        "# Select only the relevant columns: cleaned text and sentiment\n",
        "data = df[[\"Text_Limpo\", \"Sentiment_LLM\"]]\n",
        "\n",
        "# Ensure that the \"Sentiment_LLM\" and \"Text_Limpo\" columns are treated as strings\n",
        "data['Sentiment_LLM'] = data['Sentiment_LLM'].astype(str)  # Convert sentiment column to string\n",
        "data['Text_Limpo'] = data['Text_Limpo'].astype(str)  # Convert cleaned text column to string\n",
        "\n",
        "# View the resulting dataset with the selected columns\n",
        "data.head(n=20)"
      ],
      "metadata": {
        "id": "yJSxf9_SSB74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a count plot for the 'Sentiment_LLM' column, specifying the x-axis and DataFrame\n",
        "sns.countplot(x='Sentiment_LLM', data=df, palette='Set2')  # Use 'Set2' color palette\n",
        "\n",
        "# Add title and axis labels for clarity (optional)\n",
        "plt.title('Distribution of Sentiments')  # Set the title of the plot\n",
        "plt.xlabel('Sentiment')  # Label for the x-axis\n",
        "plt.ylabel('Count')  # Label for the y-axis\n",
        "\n",
        "# Display the plot without gridlines\n",
        "plt.grid(False)  # Remove gridlines\n",
        "plt.show()  # Show the plot"
      ],
      "metadata": {
        "id": "ta2xA35gSB-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Create a custom set of stopwords, updating it with additional words\n",
        "stopwords_customizadas = set(STOPWORDS)\n",
        "stopwords_customizadas.update(['python', 'twitter', 'rt'])  # Adding custom stopwords\n",
        "\n",
        "# Filter the DataFrame for each sentiment category\n",
        "sentimentos = ['Positive', 'Neutral', 'Negative']  # Adjust based on the actual values in your sentiment column\n",
        "\n",
        "# Create subsets of the DataFrame for each sentiment category\n",
        "df_positive = df[df['Sentiment_LLM'] == 'Positive']\n",
        "df_neutral = df[df['Sentiment_LLM'] == 'Neutral']\n",
        "df_negative = df[df['Sentiment_LLM'] == 'Negative']\n",
        "\n",
        "# Combine all the text from each sentiment category into a single string\n",
        "texto_positive = ' '.join(df_positive['Text_Limpo'].astype(str))  # Combine positive sentiment text\n",
        "texto_neutral = ' '.join(df_neutral['Text_Limpo'].astype(str))  # Combine neutral sentiment text\n",
        "texto_negative = ' '.join(df_negative['Text_Limpo'].astype(str))  # Combine negative sentiment text\n",
        "\n",
        "# Define additional stopwords to be excluded from the word cloud\n",
        "stopwords_customizadas = set(STOPWORDS)\n",
        "stopwords_customizadas.update(['python', 'twitter', 'rt'])  # Example of words to add to the stopword list\n"
      ],
      "metadata": {
        "id": "dECm_vTjSCBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_nuvem_palavras(texto, titulo, stopwords_customizadas, cor='viridis', salvar=False, nome_arquivo='nuvem_palavras.png'):\n",
        "    \"\"\"\n",
        "    Creates and displays a word cloud from the provided text.\n",
        "\n",
        "    Parameters:\n",
        "    texto (str): Text from which the word cloud will be generated.\n",
        "    titulo (str): Title of the word cloud chart.\n",
        "    stopwords_customizadas (set): Set of words to be excluded from the word cloud.\n",
        "    cor (str): Color palette for the word cloud.\n",
        "    salvar (bool): If True, saves the word cloud image.\n",
        "    nome_arquivo (str): Filename for saving the word cloud image.\n",
        "    \"\"\"\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color='white',\n",
        "        stopwords=stopwords_customizadas,\n",
        "        max_words=200,  # Maximum number of words to be displayed in the word cloud\n",
        "        max_font_size=100,  # Maximum font size for the largest words\n",
        "        scale=3,  # Scale for adjusting the word cloud size\n",
        "        random_state=42,  # Ensures reproducibility of the word cloud layout\n",
        "        colormap=cor  # Color map for the word cloud\n",
        "    ).generate(texto)\n",
        "\n",
        "    # Set the size of the figure\n",
        "    plt.figure(figsize=(15, 7.5))\n",
        "    # Display the word cloud with smooth interpolation\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    # Add a title to the chart\n",
        "    plt.title(titulo, fontsize=20)\n",
        "    # Hide the axis\n",
        "    plt.axis('off')\n",
        "\n",
        "    # If saving the word cloud, save it in PNG format\n",
        "    if salvar:\n",
        "        plt.savefig(nome_arquivo, format='png')\n",
        "\n",
        "    # Show the word cloud\n",
        "    plt.show()\n",
        "\n",
        "# Define neutral sentiment text\n",
        "texto_neutral = ' '.join(df_neutral['Text_Limpo'].astype(str))\n",
        "\n",
        "# Define positive sentiment text\n",
        "texto_positive = ' '.join(df_positive['Text_Limpo'].astype(str))\n"
      ],
      "metadata": {
        "id": "8g1ZU4JSSCEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the word cloud for positive sentiment\n",
        "criar_nuvem_palavras(texto_positive,\n",
        "                     'Word Cloud - Positive',\n",
        "                     stopwords_customizadas,\n",
        "                     cor='Greens',  # Use green color palette for positive sentiment\n",
        "                     salvar=True,  # Save the image\n",
        "                     nome_arquivo='nuvem_positivo.png')  # Save as 'nuvem_positivo.png'"
      ],
      "metadata": {
        "id": "ep4I6EVfSFJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the word cloud for negative sentiment\n",
        "criar_nuvem_palavras(texto_negative,\n",
        "                     'Word Cloud - Negative',\n",
        "                     stopwords_customizadas,\n",
        "                     cor='Reds',  # Use red color palette for negative sentiment\n",
        "                     salvar=True,  # Save the image\n",
        "                     nome_arquivo='nuvem_negativo.png')  # Save as 'nuvem_negativo.png'"
      ],
      "metadata": {
        "id": "ZAMY3QC6SFL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the word cloud for neutral sentiment\n",
        "criar_nuvem_palavras(texto_neutral,\n",
        "                     'Word Cloud - Neutral',\n",
        "                     stopwords_customizadas,\n",
        "                     cor='Blues',  # Use blue color palette for neutral sentiment\n",
        "                     salvar=True,  # Save the image\n",
        "                     nome_arquivo='nuvem_neutro.png')  # Save as 'nuvem_neutro.png'"
      ],
      "metadata": {
        "id": "32ChK9zJSFOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the sorted DataFrame to a new CSV file\n",
        "df.to_csv(\"dataset_final.csv\")\n",
        "print(\"Sorting completed successfully!\")"
      ],
      "metadata": {
        "id": "BGUkYoNrSFRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <div style=\"text-align:center; border-radius:15px 50px; padding:7px; color:white; margin:0; font-size:110%; font-family:Pacifico; background-color:#0073e6; overflow:hidden\"><b> Part 9 - Engineering Prompt on Dataset 2</b></div>\n",
        "\n",
        "**Objective**: Identify keywords related to climate."
      ],
      "metadata": {
        "id": "1Wa6GqMl-iH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Check if the \"Text_Limpo\" column exists\n",
        "if 'Text_Limpo' not in df.columns:\n",
        "    raise ValueError(\"The dataset must contain a column named 'Text_Limpo'.\")\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "\n",
        "print(\"Starting keyword extraction for climate-related terms...\")\n",
        "for index, row in df.iterrows():\n",
        "    text = row['Text_Limpo']\n",
        "\n",
        "    # Create a specific prompt for keyword extraction\n",
        "    prompt = (\n",
        "        f\"Identify and extract keywords related to climate or environmental topics from the following text.\\n\"\n",
        "        f\"Text: \\\"{text}\\\"\\n\"\n",
        "        f\"Keywords:\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Use the model to extract keywords (adjust according to the LLM or tool being used)\n",
        "        keywords = classifier(\n",
        "            sequences=prompt,\n",
        "            candidate_labels=[\"Climate\", \"Environment\", \"Sustainability\", \"Weather\", \"Pollution\", \"Energy\"],\n",
        "            hypothesis_template=\"This text contains keywords about {}.\"\n",
        "        )\n",
        "\n",
        "        # Collect the most relevant keywords\n",
        "        extracted_keywords = keywords['labels']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {text[:30]}... - {e}\")\n",
        "        extracted_keywords = [\"Error\"]\n",
        "\n",
        "    results.append(extracted_keywords)\n",
        "\n",
        "    # Optional: Display progress every 100 iterations\n",
        "    if (index + 1) % 100 == 0:\n",
        "        print(f\"{index + 1} texts processed...\")\n",
        "\n",
        "# Store the extracted keywords in the DataFrame\n",
        "df['Climate_Keywords'] = results\n",
        "\n",
        "print(\"Keyword extraction completed!\")"
      ],
      "metadata": {
        "id": "Bg9vLWbiSFTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column for climate-related keywords to the DataFrame\n",
        "df['Climate_Keywords'] = results  # 'results' contém as palavras-chave extraídas\n",
        "\n",
        "# Select only the relevant columns: cleaned text and keywords\n",
        "data_keywords = df[[\"Text_Limpo\", \"Climate_Keywords\"]]\n",
        "\n",
        "# Ensure that the \"Climate_Keywords\" and \"Text_Limpo\" columns are treated as strings\n",
        "\n",
        "# Convert keywords column to string\n",
        "data_keywords['Climate_Keywords'] = data_keywords['Climate_Keywords'].astype(str)\n",
        "\n",
        "# Convert cleaned text column to string\n",
        "data_keywords['Text_Limpo'] = data_keywords['Text_Limpo'].astype(str)\n",
        "\n",
        "# View the resulting dataset with the selected columns\n",
        "data_keywords.head(n=20)"
      ],
      "metadata": {
        "id": "JbKbOqRU_0X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxK67Ctlvy-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}